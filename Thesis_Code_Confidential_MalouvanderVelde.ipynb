{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7e9da6f",
   "metadata": {},
   "source": [
    "# Thesis Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "158fcc22",
   "metadata": {},
   "source": [
    "This notebook contains the code for the thesis \"Predicting Stress Recovery in Older Adults using Graph Attention Networks: a Holistic Approach\", written by Malou van der Velde. The outline is as follows:\n",
    "\n",
    "1. Preprocessing of the data\n",
    "2. Exploratory data analysis\n",
    "3. Baseline models: LIME / Error analysis\n",
    "4. Graph Attention Network: Error analysis / Visual representations of the attention weights / Tracking of model performance\n",
    "\n",
    "Given the confidentiality of the data, no output is shown in the notebook. However, the relevant output is enclosed in the thesis itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91f827e",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e61ed0",
   "metadata": {},
   "source": [
    "Importing the libraries for the preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e10a84c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re  \n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "234d4dd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df.head())\n",
    "print(df.shape)\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e807d936",
   "metadata": {},
   "source": [
    "Replace missing values 99 and 100 with actual missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3940bcee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#print unique values columns which include 99 and 100 as missing values\n",
    "print(df['Gender'].unique())\n",
    "print(df['Income'].unique())\n",
    "print(df['Education'].unique())\n",
    "\n",
    "# Define columns to exclude\n",
    "excluded_cols = [\"Age\", \"Physical_activity\"]\n",
    "\n",
    "# Apply replacement only to selected columns (99 and 100 have meaning)\n",
    "df.loc[:, ~df.columns.isin(excluded_cols)] = df.loc[:, ~df.columns.isin(excluded_cols)].replace({99: np.nan, 100: np.nan})\n",
    "\n",
    "#print unique values columns which included 99 and 100 before as a check\n",
    "print(df['Income'].unique())\n",
    "print(df['Education'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fd6905a",
   "metadata": {},
   "source": [
    "Remove cases who did not sufficiently fill in the questionnaire"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4742b269",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.shape) #sample size before\n",
    "threshold = len(df.columns) * 0.6\n",
    "df.dropna(thresh=threshold, inplace=True)\n",
    "print(df.shape) #sample size after\n",
    "#removed columns with more than 60% missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86e448ae",
   "metadata": {},
   "source": [
    "Check if there are duplicate cases that need to be removed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7c6d06a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Check duplicate cases\n",
    "duplicate_rows = df.duplicated()\n",
    "print(\"Number of duplicate rows:\", duplicate_rows.sum())\n",
    "print(\"Duplicate rows:\\n\", df[duplicate_rows])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1f3954",
   "metadata": {},
   "source": [
    "A dictionairy is created with the name of the questionnaire (as the key) and its items (as values), for the construction of the variables. Due to confidentiality, only an example is shown."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9170a6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_dict = {\n",
    "    \"openness\" : ['Personality_10', 'Personality_15', 'Personality_20', 'Personality_25', 'Personality_30'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def2fd46",
   "metadata": {},
   "source": [
    "The same is done for the demographic items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c47a2ff6",
   "metadata": {},
   "outputs": [],
   "source": [
    "demographic_dict = {\n",
    "    \"physical_act\" : ['Physical_activity']\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61290c35",
   "metadata": {},
   "source": [
    "Exclude cases who did not carefully fill in the questionnaire:\n",
    "\n",
    " the personality questionnaire contained 5 subscales and about half of the items were reverse coded, so the variance of the answer values is expected to be high. Moreover, this questionnaire was the last one on the survey, so the probability of carefully answering this questionnaire was lower. \n",
    " The variabilty of this questionnaire was therefore picked as a threshhold to exclude participants who did not fill in the questionnaire in a careful way.\n",
    " Upon further inspection of the values of these participants, one participant was exluded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babbd990",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the variance for each participant across the personality questionnaire items\n",
    "personality_items = questionnaire_dict['extraversion'] + questionnaire_dict['agreeableness'] + \\\n",
    "                    questionnaire_dict['conscientiousness'] + questionnaire_dict['neuroticism'] + \\\n",
    "                    questionnaire_dict['openness']\n",
    "\n",
    "participant_variability = df[personality_items].var(axis=1, skipna=True)\n",
    "\n",
    "# Filter participants with variability below 50\n",
    "low_variability_participants = df[participant_variability < 50]\n",
    "\n",
    "# Display the filtered participants\n",
    "print(low_variability_participants)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4c201e9",
   "metadata": {},
   "source": [
    "The questionnaire threatening experiences contained items regarding threatening experiences. The answer options were never, ever or last year. Two different variables are created: threatening experiences in the last year and ever had threatening experiences. Missing values are assumed to be indicative of not having that experience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b53de0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recode the items in the threatening experiences columns\n",
    "threatening_columns = questionnaire_dict['threatening_exp']\n",
    "#fill missing values with 0\n",
    "df[threatening_columns] = df[threatening_columns].fillna(0)\n",
    "\n",
    "# Create new columns for recoded values\n",
    "df_last_year = df[threatening_columns].applymap(lambda x: 1 if x == 1 else 0)\n",
    "df_ever = df[threatening_columns].applymap(lambda x: 1 if x == 2 else 0)\n",
    "\n",
    "# Sum of threatening experiences in the last year\n",
    "df['threatening_exp_last_year'] = df_last_year.sum(axis=1)\n",
    "\n",
    "# Sum of threatening experiences ever\n",
    "df['threatening_exp_ever'] = df_ever.sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9504cefe",
   "metadata": {},
   "source": [
    "Dictionary is adapted to include these two variables, whilst removing the now redundant variable of threatening experiences as a whole."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e6f602",
   "metadata": {},
   "outputs": [],
   "source": [
    "questionnaire_dict = {\n",
    "    \"threatening_exp_ever\": ['LTE_Ever_1', 'LTE_Ever_2', 'LTE_Ever_3', 'LTE_Ever_4', 'LTE_Ever_5', 'LTE_Ever_6', 'LTE_Ever_7', 'LTE_Ever_8', 'LTE_Ever_9', 'LTE_Ever_10', 'LTE_Ever_11', 'LTE_Ever_12'],\n",
    "    \"threatening_exp_last_year\": ['LTE_LastYear_1', 'LTE_LastYear_2', 'LTE_LastYear_3', 'LTE_LastYear_4', 'LTE_LastYear_5', 'LTE_LastYear_6', 'LTE_LastYear_7', 'LTE_LastYear_8', 'LTE_LastYear_9', 'LTE_LastYear_10', 'LTE_LastYear_11', 'LTE_LastYear_12'],\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d38f0290",
   "metadata": {},
   "source": [
    "Reverse coding the appropriate items of the questionnaires: Thought control, stress recovery, and childhood trauma. The personality questionnaire is shown as an example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ecb170",
   "metadata": {},
   "outputs": [],
   "source": [
    "display(df['Personality_1'].value_counts()) #values before reverse scoring\n",
    "\n",
    "# List of ThoughtControl items to reverse\n",
    "cols_to_reverse = ['Personality_1', 'Personality_3', 'Personality_7', 'Personality_8', 'Personality_10', 'Personality_14', 'Personality_17', 'Personality_19', 'Personality_20', 'Personality_21', 'Personality_24', 'Personality_26', 'Personality_27', 'Personality_28', 'Personality_30']\n",
    "\n",
    "# Reverse scoring (assuming a 1-5 Likert scale, where 6 - x flips the scale)\n",
    "df[cols_to_reverse] = 6 - df[cols_to_reverse]\n",
    "\n",
    "# Display the value counts for verification after reverse scoring\n",
    "display(df['Personality_1'].value_counts())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d2ef49d",
   "metadata": {},
   "source": [
    "Counting the missing values per questionnaire (also to evaluate whether person mean imputation is appropriate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9688d3b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing values for each questionnaire in questionnaire_dict\n",
    "print(\"\\nMissing values in questionnaire_dict:\")\n",
    "for key, items in questionnaire_dict.items():\n",
    "    if key in [\"threatening_exp_ever\", \"threatening_exp_last_year\"]:\n",
    "        continue  # Skip these keys\n",
    "    missing_count = df[items].isnull().sum().sum()  # Sum missing values across all items in the questionnaire\n",
    "    print(f\"{key}: {missing_count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482633e1",
   "metadata": {},
   "source": [
    "Calculating the Cronbach's alpha for reliability and thus making sure person mean imputation is an appropriate measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d3b9b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cronbach_alpha(df, items):\n",
    "    n_items = len(items)\n",
    "    \n",
    "    # There need to be at least 2 items to compute Cronbach's alpha\n",
    "    if n_items < 2:\n",
    "        return np.nan \n",
    "    \n",
    "    item_variances = df_items.var(axis=0, ddof=1)  # Variance for each item\n",
    "    total_variance = df_items.sum(axis=1).var(ddof=1)  # Variance of total score\n",
    "    \n",
    "    # Cronbach's alpha formula\n",
    "    alpha = (n_items / (n_items - 1)) * (1 - (item_variances.sum() / total_variance))\n",
    "    return alpha\n",
    "\n",
    "# Compute and store Cronbach's alpha for each questionnaire\n",
    "cronbach_results = {}\n",
    "\n",
    "for q_name, items in questionnaire_dict.items():\n",
    "    try:\n",
    "        alpha = cronbach_alpha(df, items)\n",
    "        cronbach_results[q_name] = alpha\n",
    "        print(f\"Cronbach's Alpha for {q_name}: {alpha:.3f}\")\n",
    "       \n",
    "    except Exception as e:\n",
    "        print(f\"Error calculating Cronbach's Alpha for {q_name}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c0f920c",
   "metadata": {},
   "source": [
    "Person mean imputation for the questionnaires"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbaaffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for q_name, items in questionnaire_dict.items():\n",
    "    #exclude threateing experience items from the questionnaires\n",
    "    if q_name == \"threatening_exp_ever\" or q_name == \"threatening_exp_last_year\":\n",
    "        continue\n",
    "\n",
    "    # Calculate the row-wise mean for the questionnaire, ignoring missing values\n",
    "    person_mean = df[items].mean(axis=1, skipna=True)\n",
    "    \n",
    "    # Iterate over each item in the questionnaire\n",
    "    for item in items:\n",
    "        # Impute missing values with the person mean\n",
    "        df[item] = df[item].fillna(person_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0052815",
   "metadata": {},
   "source": [
    "Check whether there are missing values left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ffa8114",
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_imputation(df, questionnaire_dict):\n",
    "    for q_name, items in questionnaire_dict.items():\n",
    "        if q_name == \"threatening_exp_ever\" or q_name == \"threatening_exp_last_year\":\n",
    "            continue\n",
    "        print(f\"Checking questionnaire: {q_name}\")\n",
    "\n",
    "        # Check for remaining missing values (Github CoPilot)\n",
    "        missing_values = df[items].isnull().sum().sum()\n",
    "        if missing_values == 0:\n",
    "            print(\"✅ No missing values remaining\")\n",
    "        else:\n",
    "            print(f\"⚠️ {missing_values} missing values still present\")\n",
    "\n",
    "# Run the function\n",
    "check_imputation(df, questionnaire_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2b63de6",
   "metadata": {},
   "source": [
    "The preprocessing of demographic variables "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c9ecfe",
   "metadata": {},
   "source": [
    "Ensuring all values of the demographic variables are floats. Age had to be converted to a float. However, participants had to fill in a number for alcohol and physical activity. Unfortunately, many gave written answers, comma's as decimals etc., so this had to be more thoroughly cleaned..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b461643",
   "metadata": {},
   "outputs": [],
   "source": [
    "#show the datatypes of all the columns in demographic_dict\n",
    "for key in demographic_dict:\n",
    "    print(key, df[demographic_dict[key]].dtypes)\n",
    "\n",
    "#convert age into float\n",
    "df['Age'] = df['Age'].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f947864a",
   "metadata": {},
   "source": [
    "Determine custom replacements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7be70ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#value counts Alcohol_Glases\n",
    "print(df['Alcohol_Glases'].value_counts())\n",
    "#sum of all the values in the column Alcohol_Glases\n",
    "print(df['Alcohol_Glases'].isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fa812fa",
   "metadata": {},
   "source": [
    "Function to clean the alcohol column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a0d803",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean and convert values to float\n",
    "def convert_to_float(value):\n",
    "    if not isinstance(value, str):  # If already a number, return it\n",
    "        return value\n",
    "    \n",
    "    # Custom replacements\n",
    "    replacements = {\n",
    "        'alleen bij gelegenheden': 1,  \n",
    "        'Nu niets meer': 0,\n",
    "        'Nog geen glas wijn per week': 0.5,\n",
    "        'glaasje in het weekend soms': 1,\n",
    "        '1 alleen op een feestje': 1,\n",
    "        '5 maar ik drink niet elke week': 5,\n",
    "        'Minder dan 1 per week, gemiddeld genomen 2 glazen per maand': 0.25,\n",
    "        '1 in de maand': 0.25,\n",
    "        'twee': 2,\n",
    "        '3-4': 3.5,\n",
    "        '2-4': 3,\n",
    "        '0 5': 0.5,\n",
    "        '15-20': 18,\n",
    "        '1/2' : 1.5\n",
    "    }\n",
    "\n",
    "    # Check for direct replacements\n",
    "    if value in replacements:\n",
    "        return replacements[value]\n",
    "    \n",
    "    # Remove unwanted words\n",
    "    words_to_remove = [\n",
    "        \"glazen\", \"glas\", \"alleen op een feestje\", \"standaardglazen\", \"drink meestal niet\", \"(één per dag)\"\n",
    "    ]\n",
    "    # Remove unwanted words\n",
    "    for word in words_to_remove:\n",
    "        value = value.replace(word, \"\").strip()\n",
    "    \n",
    "    # Convert to lowercase and normalize decimal format (convert \",\")\n",
    "    value = value.lower().replace(\",\", \".\")\n",
    "    # Extract numeric values using regex\n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", value)\n",
    "    if match:\n",
    "        return float(match.group(0))\n",
    "    \n",
    "    # Default case: return None if no match (bug)\n",
    "    return None\n",
    "\n",
    "df[\"Alcohol_Glases\"] = df[\"Alcohol_Glases\"].apply(convert_to_float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2f4eb1",
   "metadata": {},
   "source": [
    "The question was skipped if someone filled in the previous question with: I dont drink, so the missing values should be 0. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659a4121",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Alcohol_Glases'] = df['Alcohol_Glases'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536fc95e",
   "metadata": {},
   "source": [
    "The column containing the question regarding variable physical activity (the minutes per day working out) is cleaned.\n",
    "This is done in a similar way compared to the alcohol variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2bc6bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_rows', None) \n",
    "#print all the values of the column\n",
    "print(df[\"Physical_activity\"].value_counts())\n",
    "print(df[\"Physical_activity\"].unique())\n",
    "print(df[\"Physical_activity\"].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5df2bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to clean and convert values to float\n",
    "def convert_to_float(value):\n",
    "    if not isinstance(value, str):  # If already a number, return it\n",
    "        return value\n",
    "    \n",
    "    # Custom replacements for direct string-to-number mapping\n",
    "    replacements = {\n",
    "        'drie kwartier': 45,  # 45 minutes\n",
    "        '240 minuten per dag (4 uur)': 240,  # 240 minutes per day\n",
    "        'half uur tot 1 uur': 45,  # Assuming 'half hour to 1 hour' is 30 minutes\n",
    "        '1 uur': 60,\n",
    "        '2 uur': 120,\n",
    "        '3 uur': 180,\n",
    "        '300m': 300,  # Assuming '300m' is 300 minutes\n",
    "        '9 uur': 540,  # Assuming '9 uur' is 9 hours = 540 minutes\n",
    "        'sport 60 min  huis en tuin 120 min fietsen 60 min': 240,  # Summing the times\n",
    "        '1,5 uur': 90,  # 1.5 hours = 90 minutes\n",
    "        '2, 3 uur': 150,  # Assuming average of 2.5 hours = 150 minutes\n",
    "        '4 uren': 240,  # 4 hours = 240 minutes\n",
    "        '2,5': 150,  # 2.5 hours = 150 minutes\n",
    "        '2.5': 150,\n",
    "        '2,5 uur': 150,\n",
    "        '3.30': 210,  # 3.30 hours = 210 minutes\n",
    "        '11 uur = 661 min': 661,  # Explicitly provided value\n",
    "        'gemiddeld over een week genomen  tussen 60 en 90 min. tijdens fietsvakanties is dat prakties de hele dag': 75,  # Average of 60 and 90 minutes\n",
    "        '0,5': 30,  # 0.5 hours = 30 minutes\n",
    "        '1h': 60,\n",
    "    }\n",
    "\n",
    "    # Check for direct replacements\n",
    "    if value in replacements:\n",
    "        return replacements[value]\n",
    "    \n",
    "    # Remove unwanted words or phrases\n",
    "    words_to_remove = [\n",
    "        \"minuten\", \"uur\", \"m\", \"min\"\n",
    "    ]\n",
    "    for word in words_to_remove:\n",
    "        value = value.replace(word, \"\").strip()\n",
    "    \n",
    "    # Convert decimal format (convert \",\" to \".\")\n",
    "    value = value.replace(\",\", \".\")\n",
    "\n",
    "    # Extract numeric values using regex\n",
    "    match = re.search(r\"\\d+(\\.\\d+)?\", value)\n",
    "    if match:\n",
    "        return float(match.group(0))\n",
    "    \n",
    "    # Default case: return None if no match\n",
    "    return None\n",
    "\n",
    "# Apply function to the column\n",
    "df[\"Physical_activity\"] = df[\"Physical_activity\"].apply(convert_to_float)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f19f31",
   "metadata": {},
   "source": [
    "Convert the hours to minutes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d346fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hours_to_minutes(value):\n",
    "    \"\"\"\n",
    "    Converts hours into minutes for values below 11.\n",
    "    If the value is not numeric or greater than or equal to 11, it returns the original value.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Check if the value is numeric\n",
    "        numeric_value = float(value)\n",
    "        # Convert hours to minutes if the value is below 11\n",
    "        if numeric_value < 11:\n",
    "            return numeric_value * 60\n",
    "        else:\n",
    "            return value\n",
    "    except (ValueError, TypeError):\n",
    "        # Return the original value if it's not numeric\n",
    "        return value\n",
    "\n",
    "# Apply the function to the \"Physical_activity' column\n",
    "df[\"Physical_activity\"] = df[\"Physical_activity\"].apply(convert_hours_to_minutes)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e043ab7f",
   "metadata": {},
   "source": [
    "Checking for missing values in the demographic variables. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0784629",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PSQI_9', 'SF_12_Q1', 'Education', 'Income', 'SmokingAtAll', 'Gender', 'Age']].isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d666466",
   "metadata": {},
   "source": [
    "The missing values in smoking, indicates that the person does not smoke, so this is recoded as such."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89001391",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['SmokingAtAll'] = df['SmokingAtAll'].fillna(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74f96b1c",
   "metadata": {},
   "source": [
    "MICE is used to impute the missing values in education, income and physical activity. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a22c35f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the columns to impute\n",
    "columns_to_impute = ['Education', 'Income', 'Physical_activity']\n",
    "\n",
    "# Initialize the MICE imputer\n",
    "mice_imputer = IterativeImputer(random_state=0, sample_posterior=True)\n",
    "\n",
    "# Apply the imputer to the selected columns\n",
    "df[columns_to_impute] = mice_imputer.fit_transform(df[columns_to_impute])\n",
    "\n",
    "# Verify the imputed values\n",
    "for col in columns_to_impute:\n",
    "    print(f\"Unique values in {col}: {df[col].unique()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a205641",
   "metadata": {},
   "source": [
    "The full dataset is constructed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b686157",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an empty dataframe for df_all\n",
    "df_all = pd.DataFrame()\n",
    "\n",
    "# Add variables from demographic_dict to df_all using the key name\n",
    "for key, items in demographic_dict.items():\n",
    "    #just add the item itself to df_all\n",
    "    df_all[key] = df[items] \n",
    "\n",
    "# Add variables from questionnaire_dict to df_all by calculating the mean\n",
    "# Note: experiences and target are not included in the mean calculation, and are included as sum scores\n",
    "for key, items in questionnaire_dict.items():\n",
    "    if key == \"threatening_exp_ever\" or key == \"threatening_exp_last_year\":\n",
    "        continue\n",
    "    #exclude target\n",
    "    if key == \"target\":\n",
    "        continue\n",
    "    df_all[key] = df[items].mean(axis=1, skipna=True)\n",
    "\n",
    "#create sumscore of target and add to df_all\n",
    "df_all[\"target\"] = df[questionnaire_dict[\"target\"]].sum(axis=1, skipna=True)\n",
    "\n",
    "# Add the variables \"threatening_exp_ever\" and \"threatening_exp_last_year\" to df_all\n",
    "df_all[\"threatening_exp_ever\"] = df[\"threatening_exp_ever\"]\n",
    "df_all[\"threatening_exp_last_year\"] = df[\"threatening_exp_last_year\"]\n",
    "\n",
    "# Drop the original threatening_exp, to be sure \n",
    "if 'threatening_exp' in df_all.columns:\n",
    "    df_all.drop(columns=['threatening_exp'], inplace=True)\n",
    "\n",
    "#set threatening experiences ever and last year to floats\n",
    "df_all[\"threatening_exp_ever\"] = df_all[\"threatening_exp_ever\"].astype(float)\n",
    "df_all[\"threatening_exp_last_year\"] = df_all[\"threatening_exp_last_year\"].astype(float)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7c59cc2",
   "metadata": {},
   "source": [
    "Create two  sub-datasets: young adults, older adults"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb3e263",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the dataframe into two based on the Age column\n",
    "df_old = df_all[df_all['age'] > 49] \n",
    "df_young = df_all[df_all['age'] <= 49]\n",
    "\n",
    "# Display the shapes of the resulting datasets\n",
    "print(\"young:\", df_old.shape)\n",
    "print(\"old:\", df_young.shape)\n",
    "\n",
    "#mean age old and young dataset\n",
    "print(\"Mean age old dataset:\", df_old['age'].mean())\n",
    "print(\"Mean age young dataset:\", df_young['age'].mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c3c6c0d",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d637474b",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3f132a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pyreadstat\n",
    "import numpy as np  \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy.stats import skew, kurtosis\n",
    "from sklearn.manifold import TSNE\n",
    "import plotly.express as px\n",
    "from scipy.stats import pearsonr\n",
    "from minisom import MiniSom\n",
    "from scipy.stats import ttest_ind"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "047d6175",
   "metadata": {},
   "source": [
    "More informative column names are given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e544e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the new column names\n",
    "column_names = [\n",
    "    'Physical_Activity', 'Alcohol', 'Smoking', 'Income', 'Gender', 'Age',\n",
    "    'Education', 'Sleep_Quality', 'Subjective_Health', 'Thoughtcontrol',\n",
    "    'Reappraisal_Emotion', 'Suppression_Emotion', 'Neg_Interpersonal_Reg', 'Pos_Interpersonal_Reg',\n",
    "    'Rumination_Reflection', 'Rumination_Brooding', 'Resilience', 'Childhoodtrauma', 'Depression',\n",
    "    'Effective_Coping', 'Ineffective_Coping', 'Extraversion', 'Agreeableness',\n",
    "    'Conscientiousness', 'Neuroticism', 'Openness', \"Stress_Recovery\",\n",
    "    'Threatening_exp_ever', 'Threatening_exp_last_year'\n",
    "]\n",
    "\n",
    "# Rename the columns for all three datasets\n",
    "df_all.columns = column_names\n",
    "df_old.columns = column_names\n",
    "df_young.columns = column_names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058ea546",
   "metadata": {},
   "source": [
    "A distrubtion of age and the cut-off point is created, in order to display a clear destinction of the distribution between the two age groups. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81f75a79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a histogram for the 'Age' column in df_all\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Define the age threshold\n",
    "age_threshold = 50\n",
    "\n",
    "# Add a vertical red line at age 50\n",
    "plt.axvline(x=50, color='red', linestyle='--', linewidth=2, label='Age Above 50')\n",
    "\n",
    "# Separate the data into two groups\n",
    "younger = df_all[df_all['Age'] < age_threshold]['Age']\n",
    "older = df_all[df_all['Age'] >= age_threshold]['Age']\n",
    "\n",
    "# Plot the histogram for younger adults\n",
    "plt.hist(younger, bins=20, color='blue', edgecolor='black', alpha=0.7, label='Younger Adults')\n",
    "\n",
    "# Plot the histogram for older adults\n",
    "plt.hist(older, bins=20, color='orange', edgecolor='black', alpha=0.7, label='Older Adults')\n",
    "\n",
    "# Add title and labels\n",
    "plt.title(\"Distribution and Split of Age\", fontsize=16, pad=20)\n",
    "plt.xlabel(\"Age\", fontsize=12)\n",
    "plt.ylabel(\"Frequency\", fontsize=12)\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "\n",
    "# Add legend\n",
    "plt.legend(fontsize=12)\n",
    "\n",
    "# Show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54cdfc63",
   "metadata": {},
   "source": [
    "Overlapping histograms are created to get a preliminary understanding of the differences between the two age groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ee0fd06",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Iterate through each column in the DataFrame\n",
    "for column in df_young.columns:\n",
    "    if pd.api.types.is_numeric_dtype(df_young[column]) and pd.api.types.is_numeric_dtype(df_old[column]):\n",
    "        plt.figure(figsize=(8, 6))\n",
    "        \n",
    "        # Plot histogram for df_young\n",
    "        plt.hist(df_young[column], bins=30, alpha=0.5, density=True, label='Younger Adults', color='blue')\n",
    "        \n",
    "        # Plot histogram for df_old\n",
    "        plt.hist(df_old[column], bins=30, alpha=0.5, density=True, label='Older Adults', color='orange')\n",
    "        \n",
    "        # Add labels and legend\n",
    "        plt.title(f'Histogram of {column}')\n",
    "        plt.xlabel(column)\n",
    "        plt.ylabel('Proportion')\n",
    "        plt.legend()\n",
    "        \n",
    "        # Show the plot\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66e6b81a",
   "metadata": {},
   "source": [
    "This code generates histograms for each dataset seperately. They were inspected to evaluate the distribution and potential issues of the data. However, this was less informative and straightforward compared to the overlapping histograms, so this output is not included in the thesis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73921d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create histograms for all columns in a dataframe\n",
    "def create_histograms(df, title_prefix):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.hist(df[column], bins=20, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "        plt.title(f'{title_prefix} - Histogram of {column}', fontsize=14)\n",
    "        plt.xlabel(column, fontsize=12)\n",
    "        plt.ylabel('Frequency', fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# Create histograms for df_all, df_old, and df_young\n",
    "create_histograms(df_all, \"Full Dataset\")\n",
    "create_histograms(df_old, \"Older Adults\")\n",
    "create_histograms(df_young, \"Younger Adults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc289f1",
   "metadata": {},
   "source": [
    "Boxplots were created to check for issues in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36bb5309",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create boxplots for all columns in a dataframe\n",
    "def create_boxplots(df, title_prefix):\n",
    "    for column in df.columns:\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        sns.boxplot(y=df[column], color='skyblue')\n",
    "        plt.title(f'{title_prefix} - Boxplot of {column}', fontsize=14)\n",
    "        plt.ylabel(column, fontsize=12)\n",
    "        plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "        plt.show()\n",
    "\n",
    "# Create boxplots for df_all, df_old, and df_young\n",
    "create_boxplots(df_all, \"df_all\")\n",
    "create_boxplots(df_old, \"df_old\")\n",
    "create_boxplots(df_young, \"df_young\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dc0a497",
   "metadata": {},
   "source": [
    "Scatterplots were created to get a visualisation of the relationship between the predictors and the target stress recovery."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30de9c3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to create scatterplots for all columns in a dataframe with the target variable\n",
    "def create_scatterplots(df, title_prefix):\n",
    "    for column in df.columns:\n",
    "        if column != 'Stress':  # Skip the target column itself\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.scatter(df[column], df['Stress'], alpha=0.7, color='blue', edgecolor='black')\n",
    "            plt.title(f'{title_prefix} - Scatterplot of {column} vs Stress', fontsize=14)\n",
    "            plt.xlabel(column, fontsize=12)\n",
    "            plt.ylabel('Stress', fontsize=12)\n",
    "            plt.grid(linestyle='--', alpha=0.7)\n",
    "            plt.show()\n",
    "\n",
    "# Create scatterplots for df_all, df_old, and df_young\n",
    "create_scatterplots(df_all, \"Full Dataset\")\n",
    "create_scatterplots(df_old, \"Older Adults\")\n",
    "create_scatterplots(df_young, \"Younger Adults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c0962",
   "metadata": {},
   "source": [
    "This function contains heatmaps for the three datasets. Only the significant correlations are shown. The heatmaps are symmetrical and are thereby equal to amount the edges of the Graph Attention Network (the edges are bidirectionnally constructed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a626c115",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Function to calculate correlation matrix and p-values\n",
    "def calculate_correlation_and_significance(df):\n",
    "    corr_matrix = df.corr()\n",
    "    p_values = pd.DataFrame(np.zeros_like(corr_matrix), columns=df.columns, index=df.columns)\n",
    "    \n",
    "    for row in df.columns:\n",
    "        for col in df.columns:\n",
    "            if row != col:\n",
    "                _, p = pearsonr(df[row].dropna(), df[col].dropna())\n",
    "                p_values.loc[row, col] = p\n",
    "            else:\n",
    "                p_values.loc[row, col] = np.nan  # No p-value for self-correlation\n",
    "    return corr_matrix, p_values\n",
    "\n",
    "# Function to create a heatmap with significance annotations\n",
    "def create_correlation_heatmap_with_significance(df, title, alpha=0.05):\n",
    "    corr_matrix, p_values = calculate_correlation_and_significance(df)\n",
    "    plt.figure(figsize=(14, 10))  # Increase figure size for better readability\n",
    "    \n",
    "    # Mask insignificant correlations\n",
    "    mask = np.zeros_like(corr_matrix, dtype=bool)\n",
    "    mask[p_values > alpha] = True  # Mask correlations with p-value > alpha\n",
    "    \n",
    "    sns.heatmap(\n",
    "        corr_matrix, \n",
    "        annot=True, \n",
    "        fmt=\".2f\", \n",
    "        cmap=\"coolwarm\", \n",
    "        cbar=True, \n",
    "        annot_kws={\"size\": 10},  # Adjust font size of annotations\n",
    "        linewidths=0.5,  # Add lines between boxes for clarity\n",
    "        mask=mask  # Mask insignificant correlations\n",
    "    )\n",
    "    plt.xticks(rotation=45, ha=\"right\", fontsize=10)  # Rotate x-axis labels for better readability\n",
    "    plt.yticks(fontsize=10)  # Adjust font size for y-axis labels\n",
    "    plt.title(title, fontsize=16, pad=20)  # Add padding to the title\n",
    "    plt.tight_layout()  # Adjust layout to prevent clipping\n",
    "    plt.show()\n",
    "\n",
    "# Run the heatmaps for df_all, df_old, and df_young\n",
    "create_correlation_heatmap_with_significance(df_all, \"Correlation Heatmap of Significant Relationships for Both Groups\")\n",
    "create_correlation_heatmap_with_significance(df_old, \"Correlation Heatmap of Significant Relationships for Older Adults\")\n",
    "create_correlation_heatmap_with_significance(df_young, \"Correlation Heatmap of Significant Relationships for Younger Adults\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff5c2978",
   "metadata": {},
   "source": [
    "This code calculates the significant differences between the two age groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a48f800",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to calculate statistical significance for each variable\n",
    "def calculate_statistical_significance(df_old, df_young):\n",
    "    significance_results = {}\n",
    "    for column in df_old.columns:\n",
    "        if np.issubdtype(df_old[column].dtype, np.number) and np.issubdtype(df_young[column].dtype, np.number):\n",
    "            # Perform t-test\n",
    "            t_stat, p_value = ttest_ind(df_old[column].dropna(), df_young[column].dropna(), equal_var=False)\n",
    "            # Format p-value with leading 0 for better readability\n",
    "            significance_results[column] = {'t_stat': t_stat, 'p_value': f\"{p_value:.10f}\"}\n",
    "    return pd.DataFrame(significance_results).T\n",
    "\n",
    "# Calculate statistical significance\n",
    "significance_df = calculate_statistical_significance(df_old, df_young)\n",
    "\n",
    "# Display the results\n",
    "print(\"Statistical Significance Results:\")\n",
    "display(significance_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff64e2d5",
   "metadata": {},
   "source": [
    "Descriptions for all the variables in the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80864e63",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Function to calculate distributions for a dataframe\n",
    "def calculate_distributions(df):\n",
    "    stats = {}\n",
    "    for column in df.columns:\n",
    "        if np.issubdtype(df[column].dtype, np.number):  # Only process numeric columns\n",
    "            stats[column] = {\n",
    "                'min': df[column].min(),\n",
    "                'max': df[column].max(),\n",
    "                'mean': df[column].mean(),\n",
    "                'median': df[column].median(),\n",
    "                'std': df[column].std(),\n",
    "                'skewness': skew(df[column].dropna()),\n",
    "                'kurtosis': kurtosis(df[column].dropna()),\n",
    "                'count': df[column].count()\n",
    "            }\n",
    "    return pd.DataFrame(stats).T\n",
    "\n",
    "# Calculate distributions for df_all, df_old, and df_young\n",
    "distributions_all = calculate_distributions(df_all)\n",
    "distributions_old = calculate_distributions(df_old)\n",
    "distributions_young = calculate_distributions(df_young)\n",
    "\n",
    "# Display the distributions\n",
    "print(\"Distributions for df_all:\")\n",
    "display(distributions_all)\n",
    "\n",
    "print(\"Distributions for df_old:\")\n",
    "display(distributions_old)\n",
    "\n",
    "print(\"Distributions for df_young:\")\n",
    "display(distributions_young)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d8309c88",
   "metadata": {},
   "source": [
    "Demographic information of the three datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b728745",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count gender in each dataset\n",
    "gender_counts_all = df_all['Gender'].value_counts()\n",
    "gender_counts_old = df_old['Gender'].value_counts()\n",
    "gender_counts_young = df_young['Gender'].value_counts()\n",
    "\n",
    "# Display the counts\n",
    "print(\"Gender counts in df_all:\")\n",
    "print(gender_counts_all)\n",
    "\n",
    "print(\"\\nGender counts in df_old:\")\n",
    "print(gender_counts_old)\n",
    "\n",
    "print(\"\\nGender counts in df_young:\")\n",
    "print(gender_counts_young)\n",
    "\n",
    "# Calculate average and standard deviation for age in each dataset\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    avg_age = dataset['age'].mean()\n",
    "    sd_age = dataset['age'].std()\n",
    "    print(f\"Dataset: {dataset_name}\")\n",
    "    print(f\"  Average Age: {avg_age:.2f}\")\n",
    "    print(f\"  Standard Deviation of Age: {sd_age:.2f}\")\n",
    "    print(\"-\" * 30)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad89872f",
   "metadata": {},
   "source": [
    "This code creates a Self-Organizing Map for the three datasets. \n",
    "The output aligned with the t-sne regarding the amount of clustering in the three datasets. However, the interpretation was not so straightforward or clear (compared to the t-sne). Moreover, no additional information was provided by the output on top of the t-sne, so this was not included in the thesis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c18e1a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_som(data, target, dataset_name):\n",
    "    # Normalize the data\n",
    "    data_normalized = MinMaxScaler().fit_transform(data)\n",
    "\n",
    "    # SOM parameters\n",
    "    SOM_X_AXIS_NODES = 8\n",
    "    SOM_Y_AXIS_NODES = 8\n",
    "    SOM_N_VARIABLES = data_normalized.shape[1]\n",
    "    ALPHA = 0.5\n",
    "    SIGMA0 = 1.5\n",
    "    RANDOM_SEED = 123\n",
    "\n",
    "    # Initialize the SOM\n",
    "    som = MiniSom(\n",
    "        SOM_X_AXIS_NODES,\n",
    "        SOM_Y_AXIS_NODES,\n",
    "        SOM_N_VARIABLES,\n",
    "        sigma=2,  # Use an integer value for sigma\n",
    "        learning_rate=ALPHA,\n",
    "        neighborhood_function='triangle',\n",
    "        random_seed=RANDOM_SEED\n",
    "    )\n",
    "\n",
    "    # Initialize weights using PCA\n",
    "    som.pca_weights_init(data_normalized)\n",
    "\n",
    "    # Train the SOM\n",
    "    N_ITERATIONS = 5000\n",
    "    som.train_random(data_normalized, N_ITERATIONS, verbose=True)\n",
    "\n",
    "    # Plot the distance map\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(f\"Distance Map for {dataset_name}\")\n",
    "    plt.pcolor(som.distance_map().T, cmap='gist_yarg')\n",
    "    plt.colorbar()\n",
    "    plt.show()\n",
    "\n",
    "    # Plot the distance map with markers\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(f\"Distance Map with Markers for {dataset_name}\")\n",
    "    plt.pcolor(som.distance_map().T, cmap='gist_yarg')\n",
    "    plt.colorbar()\n",
    "\n",
    "    # Ensure target values are valid\n",
    "    target = target.astype(int)\n",
    "    target = target - target.min() + 1  # Shift target values to start from 1 (bug)\n",
    "\n",
    "    markers = ['o', 'x', '^', 's', 'd', '*']  # Possible to add more markers\n",
    "    colors = ['C0', 'C1', 'C2', 'C3', 'C4', 'C5']  # idem\n",
    "    #from low to high stress recovery -> blue, orange, green, red, purple, brown\n",
    "\n",
    "    for count, datapoint in enumerate(data_normalized):\n",
    "        w = som.winner(datapoint)\n",
    "        plt.plot(w[0] + 0.5, w[1] + 0.5, markers[target[count] - 1], markerfacecolor='None',\n",
    "                 markeredgecolor=colors[target[count] - 1], markersize=12, markeredgewidth=2)\n",
    "    plt.show()\n",
    "\n",
    "    # Scatter plot of winning neurons\n",
    "    w_x, w_y = zip(*[som.winner(d) for d in data_normalized])\n",
    "    w_x = np.array(w_x)\n",
    "    w_y = np.array(w_y)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.title(f\"Winning Neurons for {dataset_name}\")\n",
    "    plt.pcolor(som.distance_map().T, cmap='gist_yarg', alpha=0.2)\n",
    "    plt.colorbar()\n",
    "    for c in np.unique(target):\n",
    "        idx_target = target == c\n",
    "        plt.scatter(w_x[idx_target] + 0.5 + (np.random.rand(np.sum(idx_target)) - 0.5) * 0.8,\n",
    "                    w_y[idx_target] + 0.5 + (np.random.rand(np.sum(idx_target)) - 0.5) * 0.8,\n",
    "                    s=50,\n",
    "                    c=colors[c - 1],\n",
    "                    label=f\"Class {c}\")\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "\n",
    "# Define the datasets\n",
    "datasets = {\n",
    "    \"All Participants\": (df_all.drop(columns=[\"target\"]).values, df_all[\"target\"].values),\n",
    "    \"Older Participants\": (df_old.drop(columns=[\"target\"]).values, df_old[\"target\"].values),\n",
    "    \"Younger Participants\": (df_young.drop(columns=[\"target\"]).values, df_young[\"target\"].values)\n",
    "}\n",
    "\n",
    "# Create a SOM for each dataset\n",
    "for dataset_name, (data, target) in datasets.items():\n",
    "    create_som(data, target, dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce3616a4",
   "metadata": {},
   "source": [
    "The t-sne is applied to the three datasets and visualized in one plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd6fe3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process and visualize t-SNE for the three datasets\n",
    "def visualize_tsne(datasets, targets, dataset_names):\n",
    "    tsne_results = []\n",
    "    for i, (df, target_column) in enumerate(zip(datasets, targets)):\n",
    "        # Extract features (X) and target (y)\n",
    "        X = df.drop(columns=[target_column]).values\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        \n",
    "        # Create a DataFrame for visualization\n",
    "        tsne_df = pd.DataFrame({\n",
    "            'First T-SNE Component': X_tsne[:, 0],\n",
    "            'Second T-SNE Component': X_tsne[:, 1],\n",
    "            'Stress': y,\n",
    "            'Dataset': dataset_names[i]\n",
    "        })\n",
    "        tsne_results.append(tsne_df)\n",
    "    \n",
    "    # Combine all t-SNE results into a single DataFrame\n",
    "    combined_tsne_df = pd.concat(tsne_results, ignore_index=True)\n",
    "    \n",
    "    # Visualize the combined t-SNE results\n",
    "    fig = px.scatter(\n",
    "        combined_tsne_df,\n",
    "        x='First T-SNE Component',\n",
    "        y='Second T-SNE Component',\n",
    "        color='Stress',\n",
    "        facet_col='Dataset',\n",
    "        labels={'color': 'Stress'},\n",
    "        title=\"T-SNE Visualization of All Datasets\"\n",
    "    )\n",
    "    fig.update_layout(\n",
    "        title_font_size=20,\n",
    "        legend_title_font_size=14,\n",
    "        title={\"text\": \"T-SNE Visualization of All Datasets\", \"x\": 0.5},\n",
    "        margin=dict(l=40, r=40, t=40, b=40),\n",
    "        height=600,\n",
    "        width=1000\n",
    "    )\n",
    "    fig.show()\n",
    "\n",
    "\n",
    "# For the three datasets\n",
    "datasets = [df_all, df_young, df_old]\n",
    "targets = ['Stress', 'Stress', 'Stress']  # The target column name in each dataset\n",
    "dataset_names = ['All Participants', 'Older Adults', 'Younger Adults']  # Dataset names for the figure\n",
    "visualize_tsne(datasets, targets, dataset_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b35f6be",
   "metadata": {},
   "source": [
    "Color the t-sne visualizations by different variables to discover on which constructs the clusters in the different datasets could be based."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bae0ecd2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to process and visualize t-SNE for multiple datasets with different coloring variables\n",
    "def visualize_tsne_with_color(datasets, targets, dataset_names, color_variables):\n",
    "    for i, (df, target_column) in enumerate(zip(datasets, targets)):\n",
    "        # Extract features (X) and target (y)\n",
    "        X = df.drop(columns=[target_column]).values\n",
    "        y = df[target_column]\n",
    "        \n",
    "        # Apply t-SNE\n",
    "        tsne = TSNE(n_components=2, random_state=42)\n",
    "        X_tsne = tsne.fit_transform(X)\n",
    "        \n",
    "        # Create a DataFrame for visualization\n",
    "        tsne_df = pd.DataFrame({\n",
    "            'First T-SNE Component': X_tsne[:, 0],\n",
    "            'Second T-SNE Component': X_tsne[:, 1],\n",
    "            'Target': y\n",
    "        })\n",
    "        \n",
    "        # Add color variables to the DataFrame\n",
    "        for color_var in color_variables:\n",
    "            if color_var in df.columns:\n",
    "                tsne_df[color_var] = df[color_var]\n",
    "        \n",
    "        # Visualize the t-SNE results for each color variable\n",
    "        for color_var in color_variables:\n",
    "            if color_var in tsne_df.columns:\n",
    "                fig = px.scatter(\n",
    "                    tsne_df,\n",
    "                    x='First T-SNE Component',\n",
    "                    y='Second T-SNE Component',\n",
    "                    color=color_var,\n",
    "                    labels={'color': color_var},\n",
    "                    title=f\"T-SNE Visualization of {dataset_names[i]} Colored by {color_var}\"\n",
    "                )\n",
    "                fig.update_layout(\n",
    "                    title_font_size=20,\n",
    "                    legend_title_font_size=14,\n",
    "                    #put title in the center\n",
    "                    title={\"text\": f\"T-SNE Visualization of {dataset_names[i]} Colored by {color_var}\", \"x\": 0.5},\n",
    "                    margin=dict(l=40, r=40, t=40, b=40),\n",
    "                    height=600,\n",
    "                    width=800\n",
    "                )\n",
    "                fig.show()\n",
    "\n",
    "# Example usage\n",
    "# Assuming df_all, df_young, and df_old are your datasets\n",
    "datasets = [df_all, df_young, df_old]\n",
    "targets = ['Stress', 'Stress', 'Stress']  # Replace with the actual target column names for each dataset\n",
    "dataset_names = ['All Participants', 'Older Adults', 'Younger Adults']  # Replace with the actual dataset names\n",
    "color_variables = ['Age', 'Gender', 'Income']  # Replace with the variables you want to color by\n",
    "\n",
    "visualize_tsne_with_color(datasets, targets, dataset_names, color_variables)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8203bb5",
   "metadata": {},
   "source": [
    "## Baseline models, LIME and Error Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c056dd08",
   "metadata": {},
   "source": [
    "Importing libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afc74d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import pyreadstat\n",
    "import numpy as np  \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold, RandomizedSearchCV\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import randint\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import metrics\n",
    "import pickle\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import joblib\n",
    "import pyreadstat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "69ba74f0",
   "metadata": {},
   "source": [
    "The baseline models: Lasso regression, Support Vector Regression.\n",
    "Nested cross-validation with 3 inner and 5 outer folds \n",
    "Gridsearch for hyperparameters\n",
    "(The distribution of the target variable was checked during the construction of the baseline models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff539568",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# List of datasets\n",
    "datasets = {'df_all': df_all, 'df_old': df_old, 'df_young': df_young}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define models and their hyperparameter grids\n",
    "models = {\n",
    "    \"Lasso\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('lasso', Lasso(max_iter=10000, random_state=42))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'lasso__alpha': np.logspace(-4, 1, 10)\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', RandomForestRegressor(random_state=42))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_depth': [None, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svr', SVR())\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'svr__C': [0.1, 1, 10, 100],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate through each dataset and model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        \n",
    "        # Separate features (X) and target (y)\n",
    "        X = dataset.drop(columns=[\"target\"])  \n",
    "        y = dataset[\"target\"]  \n",
    "        \n",
    "        # Reset indices to ensure alignment\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        \n",
    "        # Nested cross-validation setup\n",
    "        outer_cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Outer loop\n",
    "        inner_cv = KFold(n_splits=3, shuffle=True, random_state=42)  # Inner loop for hyperparameter tuning\n",
    "        \n",
    "        # Perform nested cross-validation\n",
    "        outer_mae_scores = []\n",
    "        outer_mse_scores = []\n",
    "        outer_rmse_scores = []\n",
    "        outer_r2_scores = []\n",
    "        \n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_trainval, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_trainval, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Inner cross-validation for hyperparameter tuning\n",
    "            grid_search = GridSearchCV(\n",
    "                model_info[\"pipeline\"],\n",
    "                model_info[\"param_grid\"],\n",
    "                cv=inner_cv,\n",
    "                scoring='neg_mean_squared_error'\n",
    "            )\n",
    "            grid_search.fit(X_trainval, y_trainval)\n",
    "            \n",
    "            # Get the best hyperparameters from the inner fold\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            # Train the best model on the entire training set (train + validation)\n",
    "            best_model.fit(X_trainval, y_trainval)\n",
    "            \n",
    "            # Evaluate the best model on the outer test set\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate error metrics for the test set\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Append the scores for this fold\n",
    "            outer_mae_scores.append(mae)\n",
    "            outer_mse_scores.append(mse)\n",
    "            outer_rmse_scores.append(rmse)\n",
    "            outer_r2_scores.append(r2)\n",
    "            \n",
    "            print(f\"Fold MSE: {mse}\")\n",
    "            print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Calculate average and standard deviation of the error metrics\n",
    "        avg_mae = np.mean(outer_mae_scores)\n",
    "        std_mae = np.std(outer_mae_scores)\n",
    "        avg_mse = np.mean(outer_mse_scores)\n",
    "        std_mse = np.std(outer_mse_scores)\n",
    "        avg_rmse = np.mean(outer_rmse_scores)\n",
    "        std_rmse = np.std(outer_rmse_scores)\n",
    "        avg_r2 = np.mean(outer_r2_scores)\n",
    "        std_r2 = np.std(outer_r2_scores)\n",
    "        \n",
    "        # Display the average and standard deviation of the error metrics\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Average MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "        print(f\"Average RMSE: {avg_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        print(f\"Average R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "        # Store results in the list\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'dataset': dataset_name,\n",
    "            'avg_mae': avg_mae,\n",
    "            'std_mae': std_mae,\n",
    "            'avg_mse': avg_mse,\n",
    "            'std_mse': std_mse,\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'std_rmse': std_rmse,\n",
    "            'avg_r2': avg_r2,\n",
    "            'std_r2': std_r2,\n",
    "            'best_params': best_params\n",
    "        })\n",
    "\n",
    "        \n",
    "        # Save the best model for this dataset\n",
    "        model_filename = f'{model_name}_model1_{dataset_name}.pkl'\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        print(f\"Saved the best model for {dataset_name} to {model_filename}\")\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"model_comparison_results1.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved all results to 'model_comparison_results1.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a15d8537",
   "metadata": {},
   "source": [
    "Exactly the same code, except with 5 inner folds and 10 outer folds. This code is run seperately for the tracking running time (and personal inpatience). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eed0944a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets\n",
    "datasets = {'df_all': df_all, 'df_old': df_old, 'df_young': df_young}\n",
    "\n",
    "\n",
    "results = []\n",
    "\n",
    "# Define models and their hyperparameter grids\n",
    "models = {\n",
    "    \"Lasso\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('lasso', Lasso(max_iter=10000, random_state=42))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'lasso__alpha': np.logspace(-4, 1, 10)\n",
    "        }\n",
    "    },\n",
    "    \"RandomForest\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('regressor', RandomForestRegressor(random_state=42))\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'regressor__n_estimators': [50, 100, 200],\n",
    "            'regressor__max_depth': [None, 10, 20],\n",
    "            'regressor__min_samples_split': [2, 5, 10]\n",
    "        }\n",
    "    },\n",
    "    \"SVR\": {\n",
    "        \"pipeline\": Pipeline([\n",
    "            ('scaler', StandardScaler()),\n",
    "            ('svr', SVR())\n",
    "        ]),\n",
    "        \"param_grid\": {\n",
    "            'svr__C': [0.1, 1, 10, 100],\n",
    "            'svr__epsilon': [0.1, 0.2, 0.5],\n",
    "            'svr__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Iterate through each dataset and model\n",
    "for model_name, model_info in models.items():\n",
    "    print(f\"\\nModel: {model_name}\")\n",
    "    for dataset_name, dataset in datasets.items():\n",
    "        print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "        \n",
    "        # Separate features (X) and target (y)\n",
    "        X = dataset.drop(columns=[\"target\"])  \n",
    "        y = dataset[\"target\"]  \n",
    "        \n",
    "        # Reset indices to ensure alignment\n",
    "        X = X.reset_index(drop=True)\n",
    "        y = y.reset_index(drop=True)\n",
    "        \n",
    "        # Nested cross-validation setup\n",
    "        outer_cv = KFold(n_splits=10, shuffle=True, random_state=42)  # Outer loop\n",
    "        inner_cv = KFold(n_splits=5, shuffle=True, random_state=42)  # Inner loop for hyperparameter tuning\n",
    "        \n",
    "        # Perform nested cross-validation\n",
    "        outer_mae_scores = []\n",
    "        outer_mse_scores = []\n",
    "        outer_rmse_scores = []\n",
    "        outer_r2_scores = []\n",
    "        \n",
    "        for train_idx, test_idx in outer_cv.split(X, y):\n",
    "            X_trainval, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "            y_trainval, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "            \n",
    "            # Inner cross-validation for hyperparameter tuning\n",
    "            grid_search = GridSearchCV(\n",
    "                model_info[\"pipeline\"],\n",
    "                model_info[\"param_grid\"],\n",
    "                cv=inner_cv,\n",
    "                scoring='neg_mean_squared_error'\n",
    "            )\n",
    "            grid_search.fit(X_trainval, y_trainval)\n",
    "            \n",
    "            # Get the best hyperparameters from the inner fold\n",
    "            best_model = grid_search.best_estimator_\n",
    "            best_params = grid_search.best_params_\n",
    "            \n",
    "            # Train the best model on the entire training set (train + validation)\n",
    "            best_model.fit(X_trainval, y_trainval)\n",
    "            \n",
    "            # Evaluate the best model on the outer test set\n",
    "            y_pred = best_model.predict(X_test)\n",
    "            \n",
    "            # Calculate error metrics for the test set\n",
    "            mae = mean_absolute_error(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            \n",
    "            # Append the scores for this fold\n",
    "            outer_mae_scores.append(mae)\n",
    "            outer_mse_scores.append(mse)\n",
    "            outer_rmse_scores.append(rmse)\n",
    "            outer_r2_scores.append(r2)\n",
    "            \n",
    "            print(f\"Fold MSE: {mse}\")\n",
    "            print(f\"Best parameters: {best_params}\")\n",
    "        \n",
    "        # Calculate average and standard deviation of the error metrics\n",
    "        avg_mae = np.mean(outer_mae_scores)\n",
    "        std_mae = np.std(outer_mae_scores)\n",
    "        avg_mse = np.mean(outer_mse_scores)\n",
    "        std_mse = np.std(outer_mse_scores)\n",
    "        avg_rmse = np.mean(outer_rmse_scores)\n",
    "        std_rmse = np.std(outer_rmse_scores)\n",
    "        avg_r2 = np.mean(outer_r2_scores)\n",
    "        std_r2 = np.std(outer_r2_scores)\n",
    "        \n",
    "        # Display the average and standard deviation of the error metrics\n",
    "        print(f\"\\nDataset: {dataset_name}\")\n",
    "        print(f\"Average MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "        print(f\"Average MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "        print(f\"Average RMSE: {avg_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "        print(f\"Average R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "\n",
    "        # Store results in the list\n",
    "        results.append({\n",
    "            'model': model_name,\n",
    "            'dataset': dataset_name,\n",
    "            'avg_mae': avg_mae,\n",
    "            'std_mae': std_mae,\n",
    "            'avg_mse': avg_mse,\n",
    "            'std_mse': std_mse,\n",
    "            'avg_rmse': avg_rmse,\n",
    "            'std_rmse': std_rmse,\n",
    "            'avg_r2': avg_r2,\n",
    "            'std_r2': std_r2,\n",
    "            'best_params': best_params\n",
    "        })\n",
    "\n",
    "        \n",
    "        # Save the best model for this dataset\n",
    "        model_filename = f'{model_name}_model2_{dataset_name}.pkl'\n",
    "        joblib.dump(best_model, model_filename)\n",
    "        print(f\"Saved the best model for {dataset_name} to {model_filename}\")\n",
    "\n",
    "# Convert to DataFrame and save to CSV\n",
    "results_df = pd.DataFrame(results)\n",
    "results_df.to_csv(\"model_comparison_results2.csv\", index=False)\n",
    "\n",
    "print(\"\\nSaved all results to 'model_comparison_results2.csv'\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864f26c5",
   "metadata": {},
   "source": [
    "Three samples for each dataset to apply LIME to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86530e5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the sample size \n",
    "sample_fraction = 0.1 #this does mean that the full dataset shows more predictions than the other two datasets\n",
    "\n",
    "# Create sampled datasets for LIME\n",
    "df_all_sampled = df_all.sample(frac=sample_fraction, random_state=42)\n",
    "df_old_sampled = df_old.sample(frac=sample_fraction, random_state=42)\n",
    "df_young_sampled = df_young.sample(frac=sample_fraction, random_state=42)\n",
    "\n",
    "# Display the first few rows of each sampled dataset as a check\n",
    "print(\"Sampled df_all:\")\n",
    "display(df_all_sampled.head())\n",
    "\n",
    "print(\"Sampled df_old:\")\n",
    "display(df_old_sampled.head())\n",
    "\n",
    "print(\"Sampled df_young:\")\n",
    "display(df_young_sampled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b217159d",
   "metadata": {},
   "source": [
    "LIME is applied to the best models for each of the datasets "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "627cbb04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of sampled datasets and their corresponding best models\n",
    "# The best models are loaded from the saved files, but need to be picked by hand based on the previous results\n",
    "sampled_datasets = {'df_all_sampled': df_all_sampled, 'df_old_sampled': df_old_sampled, 'df_young_sampled': df_young_sampled}\n",
    "best_models = {\n",
    "    'df_all_sampled': joblib.load('Lasso_model1_df_all.pkl'),\n",
    "    'df_old_sampled': joblib.load('SVR_model2_df_old.pkl'),\n",
    "    'df_young_sampled': joblib.load('Lasso_model2_df_young.pkl')\n",
    "}\n",
    "\n",
    "# Iterate through each sampled dataset and its corresponding best model\n",
    "for name, dataset in sampled_datasets.items():\n",
    "    print(f\"Processing dataset: {name}\")\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = dataset.drop(columns=[\"target\"])  \n",
    "    y = dataset[\"target\"]  \n",
    "\n",
    "    # Use the corresponding best model\n",
    "    best_model = best_models[name]\n",
    "\n",
    "    # Predict on the dataset\n",
    "    y_pred = best_model.predict(X)\n",
    "\n",
    "    # Calculate and display the mean squared error\n",
    "    mse = mean_squared_error(y, y_pred)\n",
    "    print(f\"MSE for {name}: {mse}\")\n",
    "\n",
    "    # Scatter plot for visualization\n",
    "    plt.scatter(y, y_pred, alpha=0.7)\n",
    "    plt.plot([y.min(), y.max()], [y.min(), y.max()], '--', color='red')\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "\n",
    "    # Update the title dynamically based on the dataset name\n",
    "    dataset_title = \"Full Dataset\" if \"df_all\" in name else \"Older Adults\" if \"df_old\" in name else \"Younger Adults\"\n",
    "    plt.title(f\"True and Predicted Values for the Best Model of the {dataset_title}\")\n",
    "\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # Define the prediction function for LIME\n",
    "    def predict_fn(X):\n",
    "        return best_model.predict(X)\n",
    "\n",
    "    # Initialize LIME Explainer\n",
    "    explainer = LimeTabularExplainer(\n",
    "        training_data=X.values,  # Training data (features)\n",
    "        mode='regression',       # Regression task\n",
    "        training_labels=y.values,  # Target values\n",
    "        feature_names=X.columns.tolist(),  # Feature names\n",
    "        discretize_continuous=True  \n",
    "    )\n",
    "\n",
    "    # Choose a data point to explain (e.g., the first row of the dataset)\n",
    "    data_point_to_explain = X.iloc[0].values  # Feature vector of the data point\n",
    "\n",
    "    # Explain the prediction for the chosen data point\n",
    "    explanation = explainer.explain_instance(\n",
    "        data_point_to_explain,  # Feature vector of the data point\n",
    "        predict_fn,             # The model's prediction function\n",
    "        num_features=27         # Number of features to include in the explanation (all in this case)\n",
    "    )\n",
    "\n",
    "    # Visualize the explanation\n",
    "    explanation.show_in_notebook(show_table=True, show_all=False)\n",
    "\n",
    "    #Save the explanation as an HTML file\n",
    "    explanation.save_to_file(f'lime_explanation_{name}.html')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d914cc95",
   "metadata": {},
   "source": [
    "## Graph Attention Network\n",
    "## Error Analysis\n",
    "## Attention Weight Visualization\n",
    "## Tracking of the Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a5ed021",
   "metadata": {},
   "source": [
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6382a5f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import pickle\n",
    "import os\n",
    "from scipy.stats import loguniform\n",
    "import scipy\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from sklearn.model_selection import KFold\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "import torch\n",
    "from torch.nn import Linear, BatchNorm1d\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GATConv, global_add_pool\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import dense_to_sparse\n",
    "from torch_geometric.loader import DataLoader\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "from torchmetrics import MeanSquaredError\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import shap\n",
    "import json\n",
    "import seaborn as sns\n",
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eb97f94",
   "metadata": {},
   "source": [
    "The Graph Attention Network\n",
    "\n",
    "Architecture:\n",
    "Nodes: feature values\n",
    "Edges: significant correlations\n",
    "Nested cross-validation with 3 inner and 5 outer folds\n",
    "Random search for hyperparameters\n",
    "Loss values and attention weights are saved for later evaluation\n",
    "\n",
    "This is the entire code for the network model. However, for executing the code it is more efficient to define the model in seperate functions and then execute those."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24341c3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Set random seed for reproducibility/same seed as in baselines\n",
    "seed = 42\n",
    "\n",
    "def save_model_and_metadata(model, scaler, edge_index, best_params, y_true, y_pred, dataset_name):\n",
    "    # Ensure the directory exists\n",
    "    save_dir = f'{dataset_name}_saved_models'\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.makedirs(save_dir)\n",
    "\n",
    "    # Save the best model\n",
    "    torch.save(model, os.path.join(save_dir, f'{dataset_name}_best_gat_model_full.pth'))\n",
    "    \n",
    "    # Save preprocessing info\n",
    "    preprocessing_info = {\"scaler\": scaler, \"edge_index\": edge_index}\n",
    "    with open(os.path.join(save_dir, f'{dataset_name}_preprocessing.pkl'), 'wb') as f:\n",
    "        pickle.dump(preprocessing_info, f)\n",
    "    \n",
    "    # Save hyperparameters\n",
    "    with open(os.path.join(save_dir, f'{dataset_name}_best_hyperparams.json'), 'w') as f:\n",
    "        json.dump(best_params, f)\n",
    "    \n",
    "    # Save predictions\n",
    "    predictions_info = {\"y_true\": y_true.tolist(), \"y_pred\": y_pred.tolist()}\n",
    "    with open(os.path.join(save_dir, f'{dataset_name}_predictions.json'), 'w') as f:\n",
    "        json.dump(predictions_info, f)\n",
    "\n",
    "# Parallelized Pearson correlation calculation\n",
    "def parallel_pearson_correlation(df):\n",
    "    def calculate_pearson(col1, col2):\n",
    "        return scipy.stats.pearsonr(df[col1], df[col2])[1]\n",
    "\n",
    "    p_values = np.array(Parallel(n_jobs=-1)(\n",
    "        delayed(calculate_pearson)(col1, col2) for col1 in df.columns for col2 in df.columns\n",
    "    )).reshape(len(df.columns), len(df.columns))\n",
    "\n",
    "    return pd.DataFrame(p_values, columns=df.columns, index=df.columns)\n",
    "\n",
    "def create_graph_from_correlations(df, significance_level=0.05):\n",
    "    corr_matrix = df.corr()\n",
    "    p_values = parallel_pearson_correlation(df)  # Use the parallelized Pearson correlation\n",
    "    significant_edges = (p_values < significance_level).astype(int)\n",
    "    edge_index = dense_to_sparse(torch.tensor(significant_edges.values, dtype=torch.float))[0]\n",
    "    return edge_index\n",
    "\n",
    "#the dataset is made with a dataframe that contains the edges that should be included (based on the pearson correlation)\n",
    "def build_dataset(X, y, edge_index):\n",
    "    data_list = []\n",
    "    for i in range(len(X)):\n",
    "        x_tensor = torch.tensor(X.iloc[i].values, dtype=torch.float).unsqueeze(1)  # shape: [num_nodes, 1]\n",
    "        y_tensor = torch.tensor([y.iloc[i]], dtype=torch.float)\n",
    "        data = Data(x=x_tensor, edge_index=edge_index, y=y_tensor)\n",
    "        data_list.append(data)\n",
    "    return data_list\n",
    "\n",
    "#model architecture\n",
    "class GAT(pl.LightningModule):\n",
    "    def __init__(self, learning_rate=1e-3, optimizer_name='Adam', dropout=None, num_heads=4, n_features=10, hidden_dim=None, n_gats=0, n_fcs=0): #Default values, but overridden in the nested cross-validation\n",
    "        super(GAT, self).__init__()\n",
    "        self.learning_rate = learning_rate\n",
    "        self.optimizer_name = optimizer_name\n",
    "\n",
    "        # First GAT layer: multihead attention, then combined\n",
    "        self.gat1 = GATConv(in_channels=n_features, out_channels=hidden_dim, heads=num_heads, concat=True, dropout=dropout)\n",
    "        self.bn1 = BatchNorm1d(hidden_dim * num_heads)\n",
    "\n",
    "        # Second GAT layer\n",
    "        self.gat2 = GATConv(in_channels=hidden_dim * num_heads, out_channels=hidden_dim, heads=1, concat=False, dropout=dropout)\n",
    "        self.bn2 = BatchNorm1d(hidden_dim)\n",
    "\n",
    "        # Additional GAT layers (as tuning parameter)\n",
    "        self.gats = torch.nn.ModuleList()\n",
    "        for i in range(n_gats):\n",
    "            self.gats.append(GATConv(in_channels=hidden_dim, out_channels=hidden_dim, heads=1, concat=False, dropout=dropout))\n",
    "        \n",
    "        # Fully connected layers: now combined into one regression output\n",
    "        self.fc1 = Linear(hidden_dim, hidden_dim)\n",
    "\n",
    "        # Additional fully connected layer (as tuning parameter)\n",
    "        self.fcs = torch.nn.ModuleList()\n",
    "        self.fcs_bns = torch.nn.ModuleList()\n",
    "        for i in range(n_fcs):\n",
    "            self.fcs.append(Linear(hidden_dim, hidden_dim))\n",
    "            self.fcs_bns.append(BatchNorm1d(hidden_dim))\n",
    "\n",
    "        self.bn3 = BatchNorm1d(hidden_dim)\n",
    "        self.fc2 = Linear(hidden_dim, 1)  # Regression output\n",
    "\n",
    "        # Evaluation metrics\n",
    "        self.train_mse = MeanSquaredError()\n",
    "        self.val_mse = MeanSquaredError()\n",
    "        self.test_mse = MeanSquaredError()\n",
    "\n",
    "#The gat layers are applied, then pooled, dense layers, then output is given\n",
    "    def forward(self, data):\n",
    "        x, edge_index, batch = data.x, data.edge_index, data.batch\n",
    "\n",
    "        # First GAT layer with attention weights\n",
    "        x, (edge_index1, attn_weights1) = self.gat1(x, edge_index, return_attention_weights=True)\n",
    "        x = F.elu(x)\n",
    "        x = self.bn1(x)\n",
    "\n",
    "        x = F.elu(self.gat2(x, edge_index))\n",
    "        x = self.bn2(x)\n",
    "\n",
    "        # Apply additional GAT layers as hyperparameter\n",
    "        for gat in self.gats:\n",
    "            x = F.elu(gat(x, edge_index))\n",
    "            x = self.bn2(x)\n",
    "\n",
    "        x = global_add_pool(x, batch)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.bn3(x)\n",
    "\n",
    "        # Apply additional fully connected layers as hyperparameter\n",
    "        for fc, bn in zip(self.fcs, self.fcs_bns):\n",
    "            x = F.relu(fc(x))\n",
    "            x = bn(x)\n",
    "\n",
    "        out = self.fc2(x)  # No activation for regression\n",
    "\n",
    "        return out, attn_weights1, edge_index1\n",
    "    \n",
    "#Optimizers and loss functions are defined here\n",
    "    def configure_optimizers(self):\n",
    "        if self.optimizer_name == 'Adam':\n",
    "            optimizer = torch.optim.Adam(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'RMSprop':\n",
    "            optimizer = torch.optim.RMSprop(self.parameters(), lr=self.learning_rate)\n",
    "        elif self.optimizer_name == 'SGD':\n",
    "            optimizer = torch.optim.SGD(self.parameters(), lr=self.learning_rate)\n",
    "        else:\n",
    "            raise ValueError(f'Unsupported optimizer: {self.optimizer_name}')\n",
    "        return optimizer\n",
    "\n",
    "#mse is tracked during training, validation and testing\n",
    "    def training_step(self, train_batch, batch_idx):\n",
    "        y = train_batch.y\n",
    "        output, attn_weights1, edge_index1 = self.forward(train_batch)\n",
    "        output = output.squeeze()  # Ensure output is squeezed to match y shape\n",
    "        loss = F.mse_loss(output, y)\n",
    "        self.log('train_loss', loss, on_epoch=True, prog_bar=True, on_step=False)\n",
    "        self.log('train_mse', self.train_mse(output, y), on_epoch=True, prog_bar=True, on_step=False)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, val_batch, batch_idx):\n",
    "        y = val_batch.y\n",
    "        # print(f'{y.shape = }')\n",
    "        output, attn_weights1, edge_index1 = self.forward(val_batch) # squeeze needed or not? Check if error occurs when not using it\n",
    "        # In that case: change in each 'step' (training, validation, test, predict)\n",
    "        output = output.squeeze()  # Ensure output is squeezed to match y shape\n",
    "        # print(f'{output.shape = }')\n",
    "        loss = F.mse_loss(output, y)\n",
    "        self.log('val_loss', loss, on_epoch=True, prog_bar=True)\n",
    "        self.log('val_mse', self.val_mse(output, y), on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def test_step(self, test_batch, batch_idx):\n",
    "        y = test_batch.y\n",
    "        output, attn_weights1, edge_index1 = self.forward(test_batch)\n",
    "        output = output.squeeze()  # Ensure output is squeezed to match y shape\n",
    "        mse = self.test_mse(output, y)  # Calculate MSE\n",
    "        self.log('test_mse', mse, on_epoch=True, prog_bar=True)  # Log MSE\n",
    "        return mse\n",
    "\n",
    "    def predict_step(self, batch):\n",
    "        y_hat, attn_weights1, edge_index1 = self.forward(batch)\n",
    "        y_hat = y_hat.squeeze()\n",
    "        return y_hat, attn_weights1, edge_index1 #attention weights and edge index are returned for further analysis of gat layer \n",
    "\n",
    "#Nested crossvalidation: inner for hyperparameters (3), outer for evaluation (5)(same as for baselines)\n",
    "#The different hyperparameter combinations for the dropout, hidden dimensions, etc. are looped over\n",
    "#Epochs are set to 10, but can be changed\n",
    "def nested_cross_validation(X, y, edge_index, param_grid, n_epochs=10, outer_splits=5, inner_splits=3, dataset_name=\"dataset\"):\n",
    "    '''\n",
    "    Performs nested cross-validation for hyperparameter tuning and GAT model evaluation.\n",
    "    params:\n",
    "    X: Feature DataFrame\n",
    "    y: Target Series\n",
    "    edge_index: Edge index for graph structure\n",
    "    param_grid: List of dictionaries with hyperparameter combinations\n",
    "    n_epochs: Number of epochs for training \n",
    "    outer_splits: Number of outer folds for cross-validation\n",
    "    inner_splits: Number of inner folds for cross-validation\n",
    "    dataset_name: Name of the dataset for logging purposes\n",
    "    '''\n",
    "    outer_cv = KFold(n_splits=outer_splits, shuffle=True, random_state=seed) \n",
    "    inner_cv = KFold(n_splits=inner_splits, shuffle=True, random_state=seed)  \n",
    "\n",
    "    outer_mae_scores, outer_mse_scores, outer_rmse_scores, outer_r2_scores = [], [], [], []\n",
    "\n",
    "    best_overall_model = None\n",
    "    best_overall_score = float(\"inf\")  \n",
    "    best_params_list = []\n",
    "\n",
    "    attention_weights1_per_fold = []\n",
    "    edge_idx1_per_fold = []\n",
    "\n",
    "    for fold_idx, (trainval_idx, test_idx) in enumerate(outer_cv.split(X, y)):\n",
    "        X_trainval, X_test = X.iloc[trainval_idx], X.iloc[test_idx]\n",
    "        y_trainval, y_test = y.iloc[trainval_idx], y.iloc[test_idx]\n",
    "    \n",
    "        best_val_loss = float(\"inf\")\n",
    "        best_params = None\n",
    "        best_model = None\n",
    "\n",
    "        avg_val_losses = []\n",
    "        for params in param_grid:\n",
    "            val_losses = []\n",
    "\n",
    "            for inner_train_idx, inner_val_idx in inner_cv.split(X_trainval, y_trainval):\n",
    "                X_inner_train = X_trainval.iloc[inner_train_idx]\n",
    "                y_inner_train = y_trainval.iloc[inner_train_idx]\n",
    "                X_inner_val = X_trainval.iloc[inner_val_idx]\n",
    "                y_inner_val = y_trainval.iloc[inner_val_idx]\n",
    "\n",
    "                #Scale the features using StandardScaler\n",
    "                #Scaling the features seperately for the different folds to avoid data leakage\n",
    "                #Then same scaler is used to transform the validation set\n",
    "                scaler = StandardScaler()\n",
    "                X_inner_train_scaled = scaler.fit_transform(X_inner_train)\n",
    "                X_inner_val_scaled = scaler.transform(X_inner_val)\n",
    "\n",
    "                # Convert back to pandas DataFrame\n",
    "                X_inner_train_scaled = pd.DataFrame(X_inner_train_scaled, columns=X.columns)\n",
    "                X_inner_val_scaled = pd.DataFrame(X_inner_val_scaled, columns=X.columns)\n",
    "\n",
    "                # Build datasets\n",
    "                train_dataset = build_dataset(X_inner_train_scaled, y_inner_train, edge_index)\n",
    "                val_dataset = build_dataset(X_inner_val_scaled, y_inner_val, edge_index)\n",
    "\n",
    "                # Create DataLoader for training and validation sets\n",
    "                train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "                val_loader = DataLoader(val_dataset, batch_size=len(X_inner_val_scaled))\n",
    "\n",
    "                # Create the model with the current hyperparameters\n",
    "                model = GAT(\n",
    "                    learning_rate=params[\"learning_rate\"],\n",
    "                    optimizer_name=params[\"optimizer_name\"],\n",
    "                    dropout=params[\"dropout\"],\n",
    "                    num_heads=4,\n",
    "                    n_features=1, #This is the number of features per node, which is 1 in this case (the value of the construct)\n",
    "                    hidden_dim=params[\"hidden_dim\"],\n",
    "                    n_gats=params[\"n_gats\"],\n",
    "                    n_fcs=params[\"n_fcs\"]\n",
    "                )\n",
    "\n",
    "                \n",
    "                #train inner loops\n",
    "                trainer = pl.Trainer(max_epochs=n_epochs, accelerator=\"auto\", enable_progress_bar=False, logger=False, enable_checkpointing = False)\n",
    "                trainer.fit(model, train_loader, val_loader)\n",
    "                val_result = trainer.validate(model, dataloaders=val_loader)\n",
    "                val_loss = val_result[0]['val_loss']\n",
    "                # Only add non-NaN losses (bug)\n",
    "                if not np.isnan(val_loss):\n",
    "                    val_losses.append(val_loss)\n",
    "                else:\n",
    "                    print(f\"⚠️ Skipping config due to NaN loss: {params}\")\n",
    "\n",
    "            # Calculate the average validation loss for this hyperparameter combination\n",
    "            avg_val_loss = np.mean(val_losses)\n",
    "            avg_val_losses.append(avg_val_loss)\n",
    "            print(f\"Average inner validation MSE for params {params}: {avg_val_loss:.4f}\")\n",
    "\n",
    "        #Find best hyperparameters\n",
    "        avg_val_losses = np.array(avg_val_losses)\n",
    "        best_index = np.argmin(avg_val_losses)  # Use argmin to find the lowest validation loss\n",
    "        best_params = param_grid[best_index]  # Extract the best parameters (dict)\n",
    "        best_params_list.append(best_params)  # Store the best parameters for this fold\n",
    "        print(f\"Best parameters for fold {fold_idx + 1}: {best_params}\")\n",
    "\n",
    "        #change the parameters to those of the best model\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "        best_model = GAT(\n",
    "            learning_rate=best_params[\"learning_rate\"],\n",
    "            optimizer_name=best_params[\"optimizer_name\"],\n",
    "            dropout=best_params[\"dropout\"],\n",
    "            num_heads=4,\n",
    "            n_features=1,\n",
    "            hidden_dim=best_params[\"hidden_dim\"],\n",
    "            n_gats=best_params[\"n_gats\"],\n",
    "            n_fcs=best_params[\"n_fcs\"]\n",
    "        )\n",
    "\n",
    "        # apply the scalar to the training and test set to avoid data leakage\n",
    "        scaler = StandardScaler()\n",
    "        X_trainval_scaled = scaler.fit_transform(X_trainval)\n",
    "        X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "        # Convert back to pandas DataFrame\n",
    "        X_trainval_scaled = pd.DataFrame(X_trainval_scaled, columns=X.columns)\n",
    "        X_test_scaled = pd.DataFrame(X_test_scaled, columns=X.columns)\n",
    "        \n",
    "        # Build datasets and create DataLoader for training and test sets\n",
    "        trainval_dataset = build_dataset(X_trainval_scaled, y_trainval, edge_index)\n",
    "        trainval_loader = DataLoader(trainval_dataset, batch_size=32, shuffle=True)\n",
    "        test_dataset = build_dataset(X_test_scaled, y_test, edge_index)\n",
    "        test_loader = DataLoader(test_dataset, batch_size=len(X_test_scaled), shuffle=False)\n",
    "\n",
    "        # train outer loops\n",
    "        # logger to show mse during training later on\n",
    "        logger = CSVLogger(\"logs\", name=f'{dataset_name}') # log results to csv file\n",
    "        trainer = pl.Trainer(max_epochs=n_epochs, accelerator=\"auto\", enable_progress_bar=False, logger=logger, enable_checkpointing = False, gradient_clip_val=1.0, gradient_clip_algorithm=\"norm\") # gradient clipping to prevent exploding gradients (bug)\n",
    "        trainer.fit(best_model, trainval_loader, test_loader)\n",
    "\n",
    "        # Get output from the model (this includes the preds and attention weights)\n",
    "        output_model = trainer.predict(best_model, test_loader)\n",
    "        preds = [out[0] for out in output_model]  # Extract predictions from the output\n",
    "        attn_weights1 = [out[1] for out in output_model]\n",
    "        edge_index1 = [out[2] for out in output_model]\n",
    "        attention_weights1_per_fold.append(attn_weights1)\n",
    "        edge_idx1_per_fold.append(edge_index1)\n",
    "    \n",
    "        scaler = StandardScaler()\n",
    "        scaler.fit(y_trainval.to_numpy().reshape(-1, 1))  # Convert Series to NumPy array and reshape\n",
    "        # Now inverse transform your predictions:\n",
    "        #make numpy array of preds\n",
    "        preds = np.array(preds)  # Convert to NumPy array\n",
    "        preds = scaler.inverse_transform(preds.reshape(-1, 1))\n",
    "\n",
    "        preds = [torch.tensor(p, dtype=torch.float32) for p in preds]  # Convert to PyTorch tensors\n",
    "        # Concatenate predictions into a single array (bug)\n",
    "        preds = torch.cat([p.view(-1) for p in preds]).detach().cpu().numpy()  # Flatten predictions\n",
    "       \n",
    "        # Ensure y_test is also a NumPy array (bug)\n",
    "        y_test = y_test.to_numpy()\n",
    "\n",
    "        # Debugging: Print shapes to verify alignment (bug)\n",
    "        print(f\"Shape of preds: {preds.shape}\")\n",
    "        print(f\"Shape of y_test: {y_test.shape}\")\n",
    "\n",
    "        # Check if shapes match (bug)\n",
    "        if preds.shape[0] != y_test.shape[0]:\n",
    "            raise ValueError(f\"Shape mismatch: preds has {preds.shape[0]} samples, but y_test has {y_test.shape[0]} samples.\")\n",
    "\n",
    "        # Calculate metrics\n",
    "        mae = mean_absolute_error(y_test, preds)\n",
    "        mse = mean_squared_error(y_test, preds)\n",
    "        rmse = np.sqrt(mse)\n",
    "        r2 = r2_score(y_test, preds)\n",
    "\n",
    "        outer_mae_scores.append(mae)\n",
    "        outer_mse_scores.append(mse)\n",
    "        outer_rmse_scores.append(rmse)\n",
    "        outer_r2_scores.append(r2)\n",
    "\n",
    "        #create directory to save the models if it does not exist yet\n",
    "        if not os.path.exists(f'{dataset_name}_saved_models'):\n",
    "            os.makedirs(f'{dataset_name}_saved_models')\n",
    "\n",
    "        # Save the best model for the current fold\n",
    "        # Save the model in a specific folder for each fold\n",
    "        best_model_path = f'{dataset_name}_saved_models/{dataset_name}_fold_{fold_idx + 1}_best_model.pth'\n",
    "        torch.save(best_model.state_dict(), best_model_path) #Now they do not overwrite each other\n",
    "        # Save the entire model object \n",
    "        print(f\"Model for fold {fold_idx + 1} saved to best_fold_model.pth\")\n",
    "\n",
    "        # Track the best overall model across all folds\n",
    "        if mse < best_overall_score:  \n",
    "            best_overall_score = mse\n",
    "            best_model = best_model\n",
    "            scaler = scaler  # Save the scaler for the best model\n",
    "            best_params = best_params  # Save the best parameters for the best model\n",
    "            y_pred = preds # Save the predictions for the best model\n",
    "\n",
    "    \n",
    "    # Call save_model_and_metadata to save the best model and metadata\n",
    "    save_model_and_metadata(\n",
    "        model=best_model,\n",
    "        scaler=scaler,\n",
    "        edge_index=edge_index,\n",
    "        best_params=best_params,\n",
    "        y_true=y_test,\n",
    "        y_pred=preds,\n",
    "        dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    # Save the attention weights\n",
    "    with open(f\"{dataset_name}_all_attention_weights.pkl\", \"wb\") as f:\n",
    "        pickle.dump({\n",
    "            \"attention_weights1\": attention_weights1_per_fold,\n",
    "            \"edge_indices1\": edge_idx1_per_fold,\n",
    "        }, f)\n",
    "\n",
    "    # Calculate average and standard deviation of the error metrics across folds\n",
    "    avg_mae = np.mean(outer_mae_scores)\n",
    "    std_mae = np.std(outer_mae_scores)\n",
    "    avg_mse = np.mean(outer_mse_scores)\n",
    "    std_mse = np.std(outer_mse_scores)\n",
    "    avg_rmse = np.mean(outer_rmse_scores)\n",
    "    std_rmse = np.std(outer_rmse_scores)\n",
    "    avg_r2 = np.mean(outer_r2_scores)\n",
    "    std_r2 = np.std(outer_r2_scores)\n",
    "    \n",
    "    # Display the average and standard deviation of the error metrics\n",
    "    print(f\"\\nOuter Loop Results:\")\n",
    "    print(f\"Average MAE: {avg_mae:.4f} ± {std_mae:.4f}\")\n",
    "    print(f\"Average MSE: {avg_mse:.4f} ± {std_mse:.4f}\")\n",
    "    print(f\"Average RMSE: {avg_rmse:.4f} ± {std_rmse:.4f}\")\n",
    "    print(f\"Average R²: {avg_r2:.4f} ± {std_r2:.4f}\")\n",
    "    print(f\"Best hyperparameters: {best_params}\")\n",
    "\n",
    "\n",
    "    return outer_mae_scores, outer_mse_scores, outer_rmse_scores, outer_r2_scores, best_params_list, attention_weights1_per_fold, edge_idx1_per_fold\n",
    "\n",
    "def save_split_indices(split_indices, filename):\n",
    "    with open(filename, 'wb') as f:\n",
    "        pickle.dump(split_indices, f)\n",
    "\n",
    "def load_split_indices(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "# Load the datasets\n",
    "datasets = {\n",
    "    \"df_all\": df_all,\n",
    "    \"df_old\": df_old,\n",
    "    \"df_young\": df_young\n",
    "}\n",
    "\n",
    "\n",
    "search_space = {\n",
    "    \"dropout\": [0.0, 0.2, 0.4, 0.6, 0.8],\n",
    "    \"hidden_dim\": [16, 32, 64, 128],\n",
    "    \"learning_rate\": loguniform(1e-5, 1e-2), # Adjusted to a smaller range (otherwise bug emerges)\n",
    "    # \"learning_rate\": loguniform(1e-5, 1e-1), # Original range\n",
    "    \"optimizer_name\": ['Adam', 'SGD', 'RMSprop']\n",
    "}\n",
    "\n",
    "def generate_random_param_grid(n_samples=50, seed=seed):\n",
    "    random.seed(seed)  # Set the seed for reproducibility\n",
    "    random_param_grid = []\n",
    "    for _ in range(n_samples):\n",
    "        params = {\n",
    "            \"dropout\": random.choice(search_space[\"dropout\"]),\n",
    "            \"hidden_dim\": random.choice(search_space[\"hidden_dim\"]),\n",
    "            \"learning_rate\": search_space[\"learning_rate\"].rvs(random_state=seed),  # Pass seed for reproducibility\n",
    "            \"optimizer_name\": random.choice(search_space[\"optimizer_name\"]),\n",
    "            \"n_gats\": random.randint(0, 3),  # Randomly choose number of GAT layers (0 to 3)\n",
    "            \"n_fcs\": random.randint(0, 6)    # Randomly choose number of FC layers (0 to 6)\n",
    "        }\n",
    "        random_param_grid.append(params)\n",
    "    return random_param_grid\n",
    "\n",
    "param_grid = generate_random_param_grid(n_samples=50, seed=seed)\n",
    "print(\"Randomly selected hyperparameter combinations:\")\n",
    "for i, p in enumerate(param_grid):\n",
    "    print(f\"  Set {i+1}: {p}\")\n",
    "\n",
    "\n",
    "#executes pipeline: load data, create graph from correlations, split data, train model with nested cv, evaluate model\n",
    "# Iterate through each dataset\n",
    "results_dict = {}\n",
    "for dataset_name, dataset in datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_name}\")\n",
    "    \n",
    "    # Separate features (X) and target (y)\n",
    "    X = dataset.drop(columns=[\"target\"])  \n",
    "    y = dataset[\"target\"] \n",
    "    \n",
    "    # Create the graph structure from correlations\n",
    "    edge_index = create_graph_from_correlations(X)\n",
    "    \n",
    "    # Perform nested cross-validation\n",
    "    outer_mae_scores, outer_mse_scores, outer_rmse_scores, outer_r2_scores, best_params_list, attention_weights1_per_fold, edge_idx1_per_fold = nested_cross_validation(\n",
    "        X, y, edge_index, param_grid, n_epochs=25, outer_splits=5, inner_splits=3, dataset_name=dataset_name\n",
    "    )\n",
    "\n",
    "    # Saves the scores in a dictionary per dataset\n",
    "    results_dict[dataset_name] = {\n",
    "        \"outer_mae_scores\": outer_mae_scores,\n",
    "        \"outer_mse_scores\": outer_mse_scores,\n",
    "        \"outer_rmse_scores\": outer_rmse_scores,\n",
    "        \"outer_r2_scores\": outer_r2_scores,\n",
    "        \"best_params_list\": best_params_list\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1dd9bb",
   "metadata": {},
   "source": [
    "Error analysis for the three datasets: true versus predicted values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5646df1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# List of datasets and their corresponding titles\n",
    "datasets = {\n",
    "    \"df_all\": \"Full Dataset\",\n",
    "    \"df_old\": \"Older Adults\",\n",
    "    \"df_young\": \"Younger Adults\"\n",
    "}\n",
    "\n",
    "# Iterate through each dataset\n",
    "for dataset_name, dataset_title in datasets.items():\n",
    "    print(f\"\\nProcessing dataset: {dataset_title}\")\n",
    "    \n",
    "    # Load model\n",
    "    best_model = torch.load(f'{dataset_name}_saved_models/{dataset_name}_best_gat_model_full.pth', weights_only=False)\n",
    "    best_model.eval()\n",
    "\n",
    "    # Load preprocessing info\n",
    "    with open(f'{dataset_name}_saved_models/{dataset_name}_preprocessing.pkl', 'rb') as f:\n",
    "        preprocessing_info = pickle.load(f)\n",
    "    scaler = preprocessing_info[\"scaler\"]\n",
    "    edge_index = preprocessing_info[\"edge_index\"]\n",
    "\n",
    "    # Load predictions\n",
    "    with open(f'{dataset_name}_saved_models/{dataset_name}_predictions.json', 'r') as f:\n",
    "        predictions_info = json.load(f)\n",
    "    y_true = np.array(predictions_info[\"y_true\"])\n",
    "    y_pred = np.array(predictions_info[\"y_pred\"])\n",
    "\n",
    "    # Plot actual vs. predicted values\n",
    "    plt.scatter(y_true, y_pred, alpha=0.7)\n",
    "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], '--', color='red')\n",
    "    plt.xlabel(\"True Values\")\n",
    "    plt.ylabel(\"Predicted Values\")\n",
    "    plt.title(f\"True and Predicted Values for {dataset_title} of the GAT Model\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8958a695",
   "metadata": {},
   "source": [
    "Find the fold with the most accurate predictions to extract the attention weights from"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0816ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find the best fold based on the lowest MSE\n",
    "best_fold_index = np.argmin(outer_mse_scores)\n",
    "\n",
    "# Print details of the best fold\n",
    "print(f\"Best Fold: Fold {best_fold_index + 1}\")\n",
    "print(f\"  MAE: {outer_mae_scores[best_fold_index]:.4f}\")\n",
    "print(f\"  MSE: {outer_mse_scores[best_fold_index]:.4f}\")\n",
    "print(f\"  RMSE: {outer_rmse_scores[best_fold_index]:.4f}\")\n",
    "print(f\"  R²: {outer_r2_scores[best_fold_index]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5f2adee",
   "metadata": {},
   "source": [
    "Visualizing the attention weights in a heatmap, from which the most important attention weights are extracted to create a graph visualisation.\n",
    "(No iteration over the different datasets, so this has to be done by hand)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb958478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load attention weights from the specified path\n",
    "dataset_name = \"df_old\"  # Change this to the dataset to visualize \n",
    "attention_weights_path = f\"{dataset_name}_all_attention_weights.pkl\"\n",
    "\n",
    "with open(attention_weights_path, \"rb\") as f:\n",
    "    attention_data = pickle.load(f)\n",
    "\n",
    "#CHANGE THIS TO THE FOLD TO VISUALIZE\n",
    "# Extract the attention weights for the 4th fold (index 3) <- depends on the fold you want to visualize\n",
    "attention_weights_fold5 = attention_data[\"attention_weights1\"][3]  # 4th fold\n",
    "\n",
    "# Convert to a single tensor (averaging across all samples in this fold)\n",
    "# It is thus also possible to visualize the attention weights for one sample in the fold to uncover individual pathways\n",
    "attn_matrices = []\n",
    "for attn in attention_weights_fold5:\n",
    "    attn_tensor = attn.cpu().detach().numpy()  # Ensure the tensor is detached and converted to numpy\n",
    "    attn_matrices.append(attn_tensor)\n",
    "\n",
    "# Average attention weights across all graphs (samples) in this fold\n",
    "attn_avg = np.mean(np.stack(attn_matrices), axis=0)  # Shape: [num_edges, num_heads]\n",
    "\n",
    "# If multiple heads, average across heads too\n",
    "if attn_avg.ndim == 2:\n",
    "    attn_avg = attn_avg.mean(axis=1)\n",
    "\n",
    "# Build heatmap matrix\n",
    "num_nodes = edge_index.max() + 1  # Correctly determine the number of nodes\n",
    "heatmap_matrix = np.zeros((num_nodes, num_nodes))\n",
    "\n",
    "# Fill the heatmap with averaged attention values\n",
    "for i, (src, tgt) in enumerate(zip(edge_index[0], edge_index[1])):\n",
    "    heatmap_matrix[src, tgt] = attn_avg[i]\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "sns.heatmap(\n",
    "    heatmap_matrix,\n",
    "    cmap=\"viridis\",\n",
    "    square=True,\n",
    "    xticklabels=column_names,\n",
    "    yticklabels=column_names,\n",
    "    cbar_kws={\"label\": \"Impact Relationship on Stress Recovery\"}\n",
    ")\n",
    "plt.title(f\"Importance Attention Weights in Older Adults\") #change based on the dataset\n",
    "plt.xlabel(\"Target Node\")\n",
    "plt.ylabel(\"Source Node\")\n",
    "plt.xticks(rotation=45, ha='right') \n",
    "plt.yticks(rotation=0)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# Define the number of top attention weights to visualize\n",
    "top_k = 10\n",
    "\n",
    "# Flatten the heatmap matrix and get the indices of the top-k attention weights\n",
    "flat_indices = heatmap_matrix.flatten().argsort()[-top_k:][::-1]\n",
    "top_edges = [(i // heatmap_matrix.shape[1], i % heatmap_matrix.shape[1]) for i in flat_indices]\n",
    "top_weights = [heatmap_matrix[src, tgt] for src, tgt in top_edges]\n",
    "\n",
    "# Create a directed graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add edges with weights to the graph\n",
    "for (src, tgt), weight in zip(top_edges, top_weights):\n",
    "    G.add_edge(column_names[src], column_names[tgt], weight=weight)\n",
    "\n",
    "# Draw the graph\n",
    "plt.figure(figsize=(15, 10))\n",
    "pos = nx.spring_layout(G, seed=42)  # Layout for better visualization,\n",
    "nx.draw(\n",
    "    G, pos, with_labels=True, node_size=8000, node_color=\"lightblue\", font_size=11, font_weight=\"bold\", edge_color=\"gray\"\n",
    ")\n",
    "edge_labels = nx.get_edge_attributes(G, 'weight')\n",
    "nx.draw_networkx_edge_labels(\n",
    "    G, pos, edge_labels={k: f\"{v:.2f}\" for k, v in edge_labels.items()}, font_size=11, font_color=\"black\", font_weight=\"bold\"\t\n",
    ")\n",
    "plt.title(\"Graph of Most Important Attention Weights for Older Adults\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5824321e",
   "metadata": {},
   "source": [
    "This function plots the training curves to gain insight into how the model is learning (e.g. over/underfitting)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "375f0aef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_training_curves(logs_path, outer_mse_scores, best_params_list, dataset_name):\n",
    "    \"\"\"\n",
    "    Function to plot the training curve for the best outer fold model.\n",
    "    logs_path: Path to the folder containing the training logs.\n",
    "    outer_mse_scores: List of MSE scores for each outer fold.\n",
    "    best_params_list: List of best parameters for each outer fold.\n",
    "    dataset_name: Name of the dataset used for training.\n",
    "    \"\"\"\n",
    "    best_fold_idx = np.argmin(outer_mse_scores)  # Get the index of the best fold based on MSE\n",
    "    print(f\"Best outer fold index: {best_fold_idx}\")\n",
    "\n",
    "    # print the hyperparameters for the best fold\n",
    "    best_params = best_params_list[best_fold_idx]\n",
    "    print(f\"Best hyperparameters for fold {best_fold_idx + 1}:\")\n",
    "    print(best_params)\n",
    "    \n",
    "    # print training curve \n",
    "    curve_path = os.path.join(logs_path, f'version_{best_fold_idx}', 'metrics.csv')\n",
    "    print(f\"Training curve path: {curve_path}\")\n",
    "    logged_metrics = pd.read_csv(curve_path)\n",
    "\n",
    "    # Filter out the test metrics\n",
    "    train_indices = logged_metrics[logged_metrics['train_loss'].notna()]\n",
    "    val_indices = logged_metrics[logged_metrics['val_loss'].notna()]\n",
    "\n",
    "    # Create a figure with two subplots\n",
    "    fig, axes = plt.subplots(2, 1, figsize=(10, 12))\n",
    "\n",
    "    # Plot the mse score in the first subplot\n",
    "    axes[0].plot(train_indices['epoch'], train_indices['train_mse'], label='Train MSE')\n",
    "    axes[0].plot(val_indices['epoch'], val_indices['val_mse'], label='Validation MSE')\n",
    "    axes[0].set_xlabel('Epoch')\n",
    "    axes[0].set_ylabel('MSE')\n",
    "    axes[0].set_title(f'{dataset_name} MSE for Best Fold (fold {best_fold_idx + 1})')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # Plot the loss in the second subplot\n",
    "    axes[1].plot(train_indices['epoch'], train_indices['train_loss'], label='Train Loss')\n",
    "    axes[1].plot(val_indices['epoch'], val_indices['val_loss'], label='Validation Loss')\n",
    "    axes[1].set_xlabel('Epoch')\n",
    "    axes[1].set_ylabel('Loss')\n",
    "    axes[1].set_title(f'{dataset_name} Loss for Best Fold (fold {best_fold_idx + 1})')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "\n",
    "    # Adjust layout\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d9d024",
   "metadata": {},
   "source": [
    "The function to plot the training curves is applied to the three different models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a51e00",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the training curves for the best outer fold model for each dataset (To do: loop over other datasets)\n",
    "for dataset_name, results in results_dict.items():\n",
    "    # print the average scores and the stdev for each dataset for the outer folds (same way as baselines)\n",
    "    print(f\"\\nScores for {dataset_name}:\")\n",
    "    print(f\"Average MSE: {np.mean(results['outer_mse_scores']):.4f} ± {np.std(results['outer_mse_scores']):.4f}\")\n",
    "    print(f\"Average MAE: {np.mean(results['outer_mae_scores']):.4f} ± {np.std(results['outer_mae_scores']):.4f}\")\n",
    "    print(f\"Average RMSE: {np.mean(results['outer_rmse_scores']):.4f} ± {np.std(results['outer_rmse_scores']):.4f}\")\n",
    "    print(f\"Average R²: {np.mean(results['outer_r2_scores']):.4f} ± {np.std(results['outer_r2_scores']):.4f}\")\n",
    "\n",
    "        \n",
    "    plot_training_curves(\n",
    "        logs_path = rf'C:\\Users\\Beheerder\\Documents\\GitHub\\Thesis_Datascience\\logs\\{dataset_name}',\n",
    "        outer_mse_scores=results[\"outer_mse_scores\"],\n",
    "        best_params_list=results[\"best_params_list\"],\n",
    "        dataset_name=dataset_name\n",
    "    )"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
